{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95bdc2c0-79cf-4544-99a5-20e7ed63fd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Pea hyperspectral classification\n",
    "Patch-level training + sample-level soft voting\n",
    "Supports KMeans labels with 3 or 4 classes\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.signal import savgol_filter\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, VotingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "try:\n",
    "    from lightgbm import LGBMClassifier\n",
    "    HAS_LGBM = True\n",
    "except Exception:\n",
    "    HAS_LGBM = False\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Configuration\n",
    "# =========================\n",
    "N_CLASSES = 3  # set to 3 or 4\n",
    "\n",
    "PATCH_CSV = os.path.join(\"data\", \"pea_patch_dataset.csv\")\n",
    "LABEL_XLSX = os.path.join(\"data\", \"kmeans_labels.xlsx\")\n",
    "\n",
    "OUT_DIR = os.path.join(\"results\", f\"kmeans_{N_CLASSES}class_patch_voting\")\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "PCA_DIM = 60\n",
    "\n",
    "USE_SG = True\n",
    "SG_WINDOW = 21\n",
    "SG_POLY = 3\n",
    "SG_DERIV = 1\n",
    "\n",
    "N_SPLITS = 5\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Utility functions\n",
    "# =========================\n",
    "def snv(X):\n",
    "    mu = X.mean(axis=1, keepdims=True)\n",
    "    sd = X.std(axis=1, keepdims=True) + 1e-12\n",
    "    return (X - mu) / sd\n",
    "\n",
    "\n",
    "def sg_derivative(X, window=21, poly=3, deriv=1):\n",
    "    if window % 2 == 0:\n",
    "        window += 1\n",
    "    window = min(window, X.shape[1] - (1 - X.shape[1] % 2))\n",
    "    if window < 5:\n",
    "        window = 5\n",
    "\n",
    "    out = np.empty_like(X)\n",
    "    for i in range(X.shape[0]):\n",
    "        out[i] = savgol_filter(\n",
    "            X[i],\n",
    "            window_length=window,\n",
    "            polyorder=poly,\n",
    "            deriv=deriv\n",
    "        )\n",
    "    return out\n",
    "\n",
    "\n",
    "def load_labels(excel_path, n_classes):\n",
    "    df = pd.read_excel(excel_path, engine=\"openpyxl\")\n",
    "    cols = [c.lower().strip() for c in df.columns]\n",
    "\n",
    "    label_col = f\"kmeans_k{n_classes}_roman\"\n",
    "    roman_map = {\"i\": 1, \"ii\": 2, \"iii\": 3, \"iv\": 4}\n",
    "\n",
    "    if \"sample_id\" not in cols:\n",
    "        raise ValueError(\"sample_id column not found\")\n",
    "\n",
    "    if label_col not in cols:\n",
    "        raise ValueError(f\"{label_col} column not found\")\n",
    "\n",
    "    sid_col = df.columns[cols.index(\"sample_id\")]\n",
    "    lab_col = df.columns[cols.index(label_col)]\n",
    "\n",
    "    tmp = df[[sid_col, lab_col]].dropna().copy()\n",
    "    tmp[sid_col] = tmp[sid_col].astype(int)\n",
    "    tmp[lab_col] = tmp[lab_col].astype(str).str.lower().str.strip()\n",
    "    tmp[\"y\"] = tmp[lab_col].map(roman_map)\n",
    "    tmp = tmp.dropna(subset=[\"y\"])\n",
    "\n",
    "    if tmp[\"y\"].max() > n_classes:\n",
    "        raise ValueError(\"label exceeds class number\")\n",
    "\n",
    "    return dict(zip(tmp[sid_col], tmp[\"y\"].astype(int)))\n",
    "\n",
    "\n",
    "def aggregate_sample_probs(df_probs):\n",
    "    prob_cols = [c for c in df_probs.columns if re.fullmatch(r\"p\\d+\", c)]\n",
    "    mean_probs = df_probs.groupby(\"sample_id\")[prob_cols].mean()\n",
    "    y_pred = mean_probs.values.argmax(axis=1)\n",
    "\n",
    "    y_true = (\n",
    "        df_probs.groupby(\"sample_id\")[\"y_true\"]\n",
    "        .agg(lambda x: np.bincount(x).argmax())\n",
    "        .values\n",
    "    )\n",
    "    return y_true, y_pred\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Load data\n",
    "# =========================\n",
    "df_patch = pd.read_csv(PATCH_CSV)\n",
    "\n",
    "spec_cols = [c for c in df_patch.columns if c not in [\"patch_id\", \"sample_id\"]]\n",
    "df_patch[spec_cols] = df_patch[spec_cols].apply(pd.to_numeric, errors=\"coerce\")\n",
    "\n",
    "id2y = load_labels(LABEL_XLSX, N_CLASSES)\n",
    "\n",
    "df = df_patch[df_patch[\"sample_id\"].astype(int).isin(id2y.keys())].copy()\n",
    "df[\"y\"] = df[\"sample_id\"].astype(int).map(id2y)\n",
    "\n",
    "X_raw = df[spec_cols].values\n",
    "y_raw = df[\"y\"].values\n",
    "groups = df[\"sample_id\"].values\n",
    "\n",
    "le = LabelEncoder()\n",
    "y_enc = le.fit_transform(y_raw)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Preprocessing\n",
    "# =========================\n",
    "X = snv(X_raw)\n",
    "\n",
    "if USE_SG:\n",
    "    X = sg_derivative(X, SG_WINDOW, SG_POLY, SG_DERIV)\n",
    "\n",
    "X_std = StandardScaler().fit_transform(X)\n",
    "\n",
    "pca_dim = min(PCA_DIM, X_std.shape[1], X_std.shape[0] - 1)\n",
    "X_pca = PCA(n_components=pca_dim, random_state=RANDOM_STATE).fit_transform(X_std)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Models\n",
    "# =========================\n",
    "models = []\n",
    "\n",
    "models.append((\n",
    "    \"logreg\",\n",
    "    LogisticRegression(\n",
    "        max_iter=6000,\n",
    "        solver=\"saga\",\n",
    "        penalty=\"elasticnet\",\n",
    "        l1_ratio=0.5,\n",
    "        C=2.0,\n",
    "        class_weight=\"balanced\",\n",
    "        n_jobs=-1,\n",
    "        random_state=RANDOM_STATE\n",
    "    )\n",
    "))\n",
    "\n",
    "models.append((\n",
    "    \"rf\",\n",
    "    RandomForestClassifier(\n",
    "        n_estimators=900,\n",
    "        class_weight=\"balanced_subsample\",\n",
    "        n_jobs=-1,\n",
    "        random_state=RANDOM_STATE\n",
    "    )\n",
    "))\n",
    "\n",
    "models.append((\n",
    "    \"et\",\n",
    "    ExtraTreesClassifier(\n",
    "        n_estimators=1200,\n",
    "        class_weight=\"balanced_subsample\",\n",
    "        n_jobs=-1,\n",
    "        random_state=RANDOM_STATE\n",
    "    )\n",
    "))\n",
    "\n",
    "models.append((\n",
    "    \"xgb\",\n",
    "    XGBClassifier(\n",
    "        n_estimators=900,\n",
    "        max_depth=8,\n",
    "        learning_rate=0.05,\n",
    "        subsample=0.85,\n",
    "        colsample_bytree=0.85,\n",
    "        objective=\"multi:softprob\",\n",
    "        eval_metric=\"mlogloss\",\n",
    "        n_jobs=-1,\n",
    "        random_state=RANDOM_STATE\n",
    "    )\n",
    "))\n",
    "\n",
    "if HAS_LGBM:\n",
    "    models.append((\n",
    "        \"lgbm\",\n",
    "        LGBMClassifier(\n",
    "            objective=\"multiclass\",\n",
    "            n_estimators=1200,\n",
    "            num_leaves=63,\n",
    "            learning_rate=0.05,\n",
    "            subsample=0.85,\n",
    "            colsample_bytree=0.85,\n",
    "            class_weight=\"balanced\",\n",
    "            random_state=RANDOM_STATE\n",
    "        )\n",
    "    ))\n",
    "\n",
    "voter = VotingClassifier(estimators=models, voting=\"soft\", n_jobs=-1)\n",
    "models.append((\"voting_soft\", voter))\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Cross-validation\n",
    "# =========================\n",
    "gkf = GroupKFold(n_splits=N_SPLITS)\n",
    "\n",
    "summary = []\n",
    "best_score = (-1, -1)\n",
    "best_name = None\n",
    "best_true = None\n",
    "best_pred = None\n",
    "oof_rows = []\n",
    "\n",
    "for name, clf in models:\n",
    "    sample_true_all = []\n",
    "    sample_pred_all = []\n",
    "\n",
    "    for fold, (tr, te) in enumerate(gkf.split(X_pca, y_enc, groups), 1):\n",
    "        clf.fit(X_pca[tr], y_enc[tr])\n",
    "        proba = clf.predict_proba(X_pca[te])\n",
    "        y_hat = np.argmax(proba, axis=1)\n",
    "\n",
    "        df_fold = pd.DataFrame(proba, columns=[f\"p{i}\" for i in range(proba.shape[1])])\n",
    "        df_fold[\"sample_id\"] = groups[te]\n",
    "        df_fold[\"y_true\"] = y_enc[te]\n",
    "        df_fold[\"y_pred\"] = y_hat\n",
    "        df_fold[\"model\"] = name\n",
    "        df_fold[\"fold\"] = fold\n",
    "        oof_rows.append(df_fold)\n",
    "\n",
    "        y_true_s, y_pred_s = aggregate_sample_probs(df_fold)\n",
    "        sample_true_all.append(y_true_s)\n",
    "        sample_pred_all.append(y_pred_s)\n",
    "\n",
    "    y_true = np.concatenate(sample_true_all)\n",
    "    y_pred = np.concatenate(sample_pred_all)\n",
    "\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    f1m = f1_score(y_true, y_pred, average=\"macro\")\n",
    "\n",
    "    summary.append({\n",
    "        \"model\": name,\n",
    "        \"sample_acc\": acc,\n",
    "        \"sample_f1\": f1m\n",
    "    })\n",
    "\n",
    "    if (f1m, acc) > best_score:\n",
    "        best_score = (f1m, acc)\n",
    "        best_name = name\n",
    "        best_true = y_true\n",
    "        best_pred = y_pred\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Save results\n",
    "# =========================\n",
    "pd.DataFrame(summary).sort_values(\n",
    "    [\"sample_f1\", \"sample_acc\"], ascending=False\n",
    ").to_csv(\n",
    "    os.path.join(OUT_DIR, \"model_comparison.csv\"),\n",
    "    index=False\n",
    ")\n",
    "\n",
    "oof_df = pd.concat(oof_rows, ignore_index=True)\n",
    "oof_df.to_csv(\n",
    "    os.path.join(OUT_DIR, \"oof_patch_predictions.csv\"),\n",
    "    index=False\n",
    ")\n",
    "\n",
    "report = classification_report(\n",
    "    le.inverse_transform(best_true),\n",
    "    le.inverse_transform(best_pred),\n",
    "    output_dict=True,\n",
    "    digits=4\n",
    ")\n",
    "pd.DataFrame(report).T.to_csv(\n",
    "    os.path.join(OUT_DIR, f\"{best_name}_classification_report.csv\")\n",
    ")\n",
    "\n",
    "cm = confusion_matrix(\n",
    "    le.inverse_transform(best_true),\n",
    "    le.inverse_transform(best_pred),\n",
    "    labels=le.classes_\n",
    ")\n",
    "pd.DataFrame(cm).to_csv(\n",
    "    os.path.join(OUT_DIR, f\"{best_name}_confusion_matrix.csv\"),\n",
    "    index=False\n",
    ")\n",
    "\n",
    "print(\"Best model:\", best_name)\n",
    "print(\"Accuracy:\", round(best_score[1], 4))\n",
    "print(\"Macro F1:\", round(best_score[0], 4))\n",
    "print(\"Results saved to:\", OUT_DIR)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
